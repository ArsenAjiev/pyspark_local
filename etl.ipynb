{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a04d54-01be-499e-97b8-7c445c8dc643",
   "metadata": {},
   "source": [
    "Задача. Сделать пайплайн обработки файла cars.csv.\n",
    "\n",
    "Необходимо посчитать по каждому производителю (поле manufacturer_name):\n",
    "\n",
    "кол-во объявлений\n",
    "средний год выпуска автомобилей\n",
    "минимальную цену\n",
    "максимальную цену\n",
    "Выгрузить результат в output.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3607f13-b0c5-4af8-9132-054ab2baf067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/06 16:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/jovyan/work/data/cars.csv",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 46\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;66;03m# LOAD\u001B[39;00m\n\u001B[1;32m     41\u001B[0m     output\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/cars_result1.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 46\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 14\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# EXTRACT\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# read data\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mheader\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/jovyan/work/data/cars.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# check read data\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# df.show(5)\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m#TRANSFORM\u001B[39;00m\n\u001B[1;32m     20\u001B[0m output \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     21\u001B[0m     df\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# считаем по каждому производителю\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m \n\u001B[1;32m     35\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/pyspark_local/venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:177\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 177\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/pyspark_local/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/PycharmProjects/pyspark_local/venv/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    192\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: file:/home/jovyan/work/data/cars.csv"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1 create spark session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # EXTRACT\n",
    "    # read data\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/home/jovyan/work/data/cars.csv\")\n",
    "    # check read data\n",
    "    # df.show(5)\n",
    "    \n",
    "    #TRANSFORM\n",
    "    \n",
    "    output = (\n",
    "        df\n",
    "        # считаем по каждому производителю\n",
    "        .groupBy(\"manufacturer_name\")\n",
    "        .agg(\n",
    "            # первая метрика - количество авто\n",
    "            F.count(\"manufacturer_name\").alias (\"count\"),\n",
    "            # вторая метрика  - средний год выпуска автомобилей\n",
    "            F.round(F.avg(\"year_produced\")).cast(t.IntegerType()).alias(\"avg_year\"),\n",
    "            # третья метрика - минимальная цена\n",
    "            F.min(F.col(\"price_usd\").cast(t.FloatType())).alias(\"min_price\"),\n",
    "            # четвертая метрика - максимальная цена\n",
    "            F.max(F.col(\"price_usd\").cast(t.FloatType())).alias(\"max_price\")\n",
    "        )\n",
    "    \n",
    "    )\n",
    "    \n",
    "    output.show()\n",
    "    \n",
    "    # LOAD\n",
    "    \n",
    "    output.coalesce(5).write.mode(\"overwrite\").format(\"json\").save(\"data/cars_result1.json\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d5eb4-cf28-4155-b0e3-d052d8c4522d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
